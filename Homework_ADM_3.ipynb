{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Find the perfect place to stay in Texas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/francois/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/francois/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/francois/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import nltk.stem as stemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert csv file to tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcsv = open('Airbnb_Texas_Rentals.csv','r', errors='ignore')\n",
    "ftsv = open('Airbnb_Texas_Rentals.tsv','w', errors='ignore')\n",
    "with fcsv as csvin, ftsv as tsvout:\n",
    "    csvin = csv.reader(csvin)\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "\n",
    "    for row in csvin:\n",
    "        tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below preprocess data by deleting all the stopwords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokme(query):\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "    tokens = [i.lower() for i in tokens if ( i not in string.punctuation and i not in stop_words)]\n",
    "    tokens = [i.replace(\" \", \"\") for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conjunctive query***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates dictionary where store the whole set of preprocessed words used \n",
    "#in listings as keys and docs where they are used as values\n",
    "#It is our inverted index\n",
    "def createWordDict(row, doc_name, wordDict):\n",
    "    row = row[5]+row[8]\n",
    "    replaced = re.sub(r'[\\/\\.]', ' ', row.replace(\"\\\\n\",\" \"))\n",
    "    row = list(set(tokme(replaced)))\n",
    "    for word in row:\n",
    "        if word in wordDict :\n",
    "            wordDict[word].append(doc_name)\n",
    "        else:\n",
    "            wordDict[word] = [doc_name]\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function create TSV file for each row\n",
    "def createTSV(row, doc_name):\n",
    "    tsvout = open('Docs/'+doc_name,'w', errors='ignore')\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "    tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using this function we save inverted index\n",
    "#Also we create a vocabulary for all the words used in listings\n",
    "def saveVoc(wordDict):\n",
    "    with open('wordDict.json', 'w') as fp:\n",
    "        json.dump(wordDict, fp)\n",
    "    with open('vocabulary.csv', 'w') as fv:\n",
    "        wr = csv.writer(fv, quoting=csv.QUOTE_ALL)\n",
    "        vocabulary = wordDict.keys()\n",
    "        wr.writerow(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function preprocesses the query\n",
    "def getQuery(query):\n",
    "    listQuery = tokme(query)\n",
    "    return(listQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using this function we create a final list of the documents.\n",
    "#This will be an output of the first engine\n",
    "def searchQuery(query, wordDict):\n",
    "    result = []\n",
    "    wordsNotFound = []\n",
    "    # case word not in dict\n",
    "    for word in query:\n",
    "        if word not in wordDict:\n",
    "            wordsNotFound.append(word)\n",
    "            continue\n",
    "        if result != []:\n",
    "            result = list(set(wordDict[word]) & set(result))\n",
    "        else:\n",
    "            result = wordDict[word]\n",
    "    return(result, wordsNotFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using this function we print the output of our query\n",
    "def printResult(results, wordsNotFound):\n",
    "    for r in results:\n",
    "        df=pd.read_csv('Docs/'+r, sep='\\t', usecols=[3, 5, 8, 9], header=None)\n",
    "        print(df)\n",
    "    if wordsNotFound != []:\n",
    "        print (\"Words that the documents don't contain : \" + \", \".join(wordsNotFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a main function where we preprocess the data, create an inverted index and save it.\n",
    "def preprocessingData():\n",
    "    fcsv = open('Airbnb_Texas_Rentals.csv','r',errors='ignore')\n",
    "    wordDict = {}\n",
    "    with fcsv as csvin:\n",
    "        csvin = csv.reader(csvin)\n",
    "        for i,row in enumerate(csvin):\n",
    "            doc_name = \"doc_\"+str(i)+\".tsv\"\n",
    "            createTSV(row, doc_name)\n",
    "            wordDict = createWordDict(row, doc_name, wordDict)      \n",
    "    saveVoc(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READS INVERTED INDEX FROM JSON FILE\n",
    "def loadJSONDict():\n",
    "    json1_file = open('wordDict.json')\n",
    "    json1_str = json1_file.read()\n",
    "    json1_data = json.loads(json1_str)\n",
    "    return json1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE FIRST SEARCH ENGINE.\n",
    "# HERE WE READ the inverted index from JSON file\n",
    "# INSERT QUERY\n",
    "# AND PRINT THE RESULT OF THE SEARCH\n",
    "def SearchEngine(query):\n",
    "    wordDict = loadJSONDict()\n",
    "    #print(wordDict)\n",
    "    query = getQuery(query)\n",
    "    results, wordsNotFound = searchQuery(query, wordDict)\n",
    "    printResult(results, wordsNotFound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input()\n",
    "SearchEngine(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conjunctive query & Ranking score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTSV2(row, doc_name):\n",
    "    \"\"\"Save our row (apartment) in a tsv file \"\"\"\n",
    "    tsvout = open('Docs/'+doc_name,'w', errors='ignore')\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "    tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordDict2(row, doc_name, wordDict, distDocDict):\n",
    "    \"\"\"Fill the inverted index and the dictionary of distance of every documents\n",
    "    Input : row\n",
    "            doc_name : document name\n",
    "            wordDict : empty dictionary, inverted index empty\n",
    "            distDocDict : empty dictionary , will contain the distance of every documents\n",
    "    \n",
    "    Output : wordDict : dictionary, the inverted index\n",
    "             distDocDict : dictionary contain the distance of every documents\"\"\"\n",
    "    # we select only the description and the title\n",
    "    row = row[5]+row[8]\n",
    "    # we replace '\\','.','\\\\n' by a space\n",
    "    replaced = re.sub(r'[\\/\\.]', ' ', row.replace(\"\\\\n\",\" \"))\n",
    "    row = tokme(replaced)\n",
    "    for word in row:\n",
    "        # fill the dictionary of distances\n",
    "        if doc_name in distDocDict:\n",
    "            distDocDict[doc_name].append(row.count(word))\n",
    "        else:\n",
    "            distDocDict[doc_name] = [row.count(word)]\n",
    "        # fill the inverted index\n",
    "        if word in wordDict :\n",
    "        # create a tuple : (document name, number of time the word appear in the doc, 0 but will be replace by the TFIDF)\n",
    "            if wordDict[word][-1][0] == doc_name: # if last doc of the list for the word is the current doc\n",
    "                # add 1 to the number of time the word appear in the doc\n",
    "                wordDict[word][-1] = (wordDict[word][-1][0], wordDict[word][-1][1]+1, 0)\n",
    "            else:\n",
    "                # create the tuple\n",
    "                wordDict[word].append((doc_name, 1, 0))#doc_name, nb of word, FdIdf\n",
    "        else:\n",
    "            wordDict[word] = [(doc_name, 1, 0)] #doc_name, nb of word, FdIdf\n",
    "    return (wordDict, distDocDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTFIDF(wordDict, nbDoc):\n",
    "    \"\"\"Calculate the TFIDF for every word and every document\n",
    "    Input : \n",
    "        wordDict : inverted index without the TFIDF\n",
    "        nbDoc : number of documents\n",
    "    \n",
    "    Output : \n",
    "        wordDict : inverted index with the TFIDF\"\"\"\n",
    "    for word, listDoc in wordDict.items():\n",
    "        # for every list of tuple (document name, number of apearence of the word in the document, TFIDF)\n",
    "        listDoc2 = [(t[0], t[1], math.log10(nbDoc/len(listDoc))*t[1]) for t in listDoc]\n",
    "        wordDict[word] = listDoc2\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDistDoc(distDocDict):\n",
    "    \"\"\"Calculate the distance of every documents\n",
    "    Input : Dictionary \n",
    "            key : document name\n",
    "            value : list (int) of number of apearance of every word in the document\n",
    "            \n",
    "    Output : Dictionary\n",
    "            key : document name\n",
    "            value : distance of the document\"\"\"\n",
    "    distDocDict2 = {doc : math.sqrt(sum(map(lambda x:x**2,l))) for doc,l in distDocDict.items()}\n",
    "    return distDocDict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveVoc2(wordDict):\n",
    "    \"\"\"Save the inverted index and the vocabulary\"\"\"\n",
    "    # save the inverted index to json\n",
    "    with open('wordDict2.json', 'w') as fp:\n",
    "        json.dump(wordDict, fp)\n",
    "    # save all the words in a vocabulary file\n",
    "    with open('vocabulary2.csv', 'w') as fv:\n",
    "        wr = csv.writer(fv, quoting=csv.QUOTE_ALL)\n",
    "        vocabulary = wordDict.keys()\n",
    "        wr.writerow(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDocDist(distDocDict):\n",
    "    \"\"\"Save the dictionary of the distance of the documents\"\"\"\n",
    "    with open('distDocDict.json', 'w') as fp:\n",
    "        json.dump(distDocDict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingData2():\n",
    "    \"\"\"Preprocessing the data\"\"\"\n",
    "    fcsv = open('Airbnb_Texas_Rentals.csv','r',errors='ignore')\n",
    "    wordDict = {} # inverted index\n",
    "    distDocDict = {} # dictionary of the distance for every file\n",
    "    nbDoc = 0 # number of documents\n",
    "    with fcsv as csvin:\n",
    "        csvin = csv.reader(csvin)\n",
    "        for i,row in enumerate(csvin):\n",
    "            # we don't take the first line of the CSV file\n",
    "            if i == 0: continue\n",
    "            doc_name = \"doc_\"+str(i)+\".tsv\"\n",
    "            createTSV2(row, doc_name) # create the TSV files\n",
    "            wordDict, distDocDict = createWordDict2(row, doc_name, wordDict, distDocDict) # fill the dictionaries\n",
    "            nbDoc = i\n",
    "        wordDict = calcTFIDF(wordDict, nbDoc) # compute the TFIDF\n",
    "        distDocDict = calcDistDoc(distDocDict) # compute the distance of every documents\n",
    "    saveVoc2(wordDict) # save the inverted index\n",
    "    saveDocDist(distDocDict) # save the dictionary of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessingData2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuery2(query):\n",
    "    \"\"\"Remove ponctuation and stopwords from the query and return it under the form of a list\"\"\"\n",
    "    listQuery = tokme(query)\n",
    "    return(listQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadJSONDict2():\n",
    "    \"\"\"Load the inverted index\"\"\"\n",
    "    json1_file = open('wordDict2.json')\n",
    "    json1_str = json1_file.read()\n",
    "    json1_data = json.loads(json1_str)\n",
    "    return json1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDocDist():\n",
    "    \"\"\"Load the file containing the distances\"\"\"\n",
    "    json1_file = open('distDocDict.json')\n",
    "    json1_str = json1_file.read()\n",
    "    json1_data = json.loads(json1_str)\n",
    "    return json1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDist(listnbWord):\n",
    "    \"\"\"return the sum of square of all the element in a list. Use to calculate the distance of the query\n",
    "    Input : list of int\n",
    "    \n",
    "    Output : int\"\"\"\n",
    "    return math.sqrt(sum(map(lambda x:x**2,listnbWord)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchQuery2(query, wordDict):\n",
    "    \"\"\"We search every documents that contain every word of the query. If a word is not in the documents, \n",
    "    we put it in a list and continue the search\n",
    "    \n",
    "    Input : query, inverted index\n",
    "    \n",
    "    Output : \n",
    "        result : list of documents that contain every word of the query\n",
    "        wordsNotFound : list of the words of the query that are not in the inverted index\n",
    "        distQuery : distance of the query\"\"\"\n",
    "    result = [] # list of documents that contain every word of the query\n",
    "    wordsNotFound = [] # list of the words of the query that are not in the inverted index\n",
    "    nbWord = [] # list of the count of every word in the query\n",
    "    querySet = list(set(query))\n",
    "    for word in querySet:\n",
    "        nbWord.append(query.count(word))\n",
    "         # case word not in dict\n",
    "        if word not in wordDict:\n",
    "            wordsNotFound.append(word)\n",
    "            continue\n",
    "        if result != []:\n",
    "            # get every document name of the list of the world in the inverted index\n",
    "            lDoc = [t[0] for t in wordDict[word]]\n",
    "            # the intersection between the list and the result\n",
    "            result = list(set(lDoc) & set(result))\n",
    "        else:\n",
    "            result = [t[0] for t in wordDict[word]]\n",
    "    # Compute the distance of the query\n",
    "    distQuery = calcDist(nbWord)\n",
    "    return(result, wordsNotFound, distQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAllCosineSim(results, query, distQuery, wordDict, distDocDict):\n",
    "    \"\"\"Compute the cosine similarity between the query and all the documents of the result\n",
    "    Input : query\n",
    "            distQuery : distance of the query\n",
    "            wordDict : inverted index\n",
    "            distDocDict : dictionary of the distance of every documents\n",
    "            \n",
    "    Output : dictionary of the cosine similarity (key : document name, value cosine similarity)\"\"\"\n",
    "    cosineSimDict = {} # dictionary with key is the document name and the value is the cosine similarity\n",
    "    for doc in results:\n",
    "        lValue = sum([wordDict[word] for word in list(set(query))], [])\n",
    "        TFIDF = [t[2] for t in lValue if t[0]==doc] # get the TFIDF of the document\n",
    "        cosineSim = calcCosineSim(TFIDF, distDocDict[doc], distQuery) # compute the cosine similarity\n",
    "        cosineSimDict[doc] = cosineSim # put it in the dictionary\n",
    "    return cosineSimDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCosineSim(TFIDF, distDoc, distQuery):\n",
    "    \"\"\"return the cosine similarity between a document and the query\"\"\"\n",
    "    return(sum(TFIDF)/(distDoc*distQuery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult2(results, wordsNotFound, cosineSimDict):\n",
    "    \"\"\"Print the best k results (here k=10) and the worlds not found\"\"\"\n",
    "    # Create a DataFrame which will contains all the results\n",
    "    df = pd.DataFrame(index=range(len(results)), columns=[\"City\", \"Description\", \"Title\", \"Link\", \"cosineSim\"])\n",
    "    for i,r in enumerate(results):\n",
    "        # load the document in a DataFrame (dfd)\n",
    "        dfd = pd.read_csv('Docs/'+r, sep='\\t', usecols=[3, 5, 8, 9], header=None, names=[\"City\", \"Description\", \"Title\", \"Link\"], encoding=\"latin-1\")\n",
    "        dfd['cosineSim'] = pd.Series(cosineSimDict[r]) # add the cosine similarity to the dfd\n",
    "        df.loc[i] = dfd.loc[0] # add it to the big DataFrame\n",
    "    df = df.sort_values(by='cosineSim', ascending=False) # Sort the DataFrame by the cosine Similarity\n",
    "    print(df.head(10)) # print the k best results, here k=10\n",
    "    # print the words that have not been found\n",
    "    if wordsNotFound != []:\n",
    "        print (\"Words that the documents don't contain : \" + \", \".join(wordsNotFound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine2(query):\n",
    "    \"\"\"Search the result for a query and print it\n",
    "    Input : query\"\"\"\n",
    "    wordDict = loadJSONDict2() # load the inverted index\n",
    "    distDocDict = loadDocDist() # load the dictionary of the distances\n",
    "    query = getQuery2(query)\n",
    "    results, wordsNotFound, distQuery = searchQuery2(query, wordDict) # research for the query\n",
    "    # calculate the cosine similarity between the documents of the result and the query\n",
    "    cosineSimDict = calcAllCosineSim(results, query, distQuery, wordDict, distDocDict)\n",
    "     # print the result\n",
    "    printResult2(results, wordsNotFound, cosineSimDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a beautiful house with garden\n",
      "           City                                        Description  \\\n",
      "10       Austin  A beautiful, modern home, surrounded by a beau...   \n",
      "13       Austin  A beautiful, modern home, surrounded by a beau...   \n",
      "15  Duncanville  Welcome to Alla's Garden House! \\nBeautiful pr...   \n",
      "7        Spring  Attractions: The Woodlands, incredible views, ...   \n",
      "6        Spring  Beautiful 2100 sq. ft house! Three bedroom and...   \n",
      "11       Dallas  Backyard paradise close to Plano. I have creat...   \n",
      "4    Fort Worth  Explore the City of Cowboys and Culture from t...   \n",
      "8    Fort Worth  Explore the City of Cowboys and Culture from t...   \n",
      "1       Helotes  Renovated historic barn with concrete floors, ...   \n",
      "0          Kyle  Three room house situated in the Hill Country ...   \n",
      "\n",
      "                                                Title  \\\n",
      "10   Stay at an architect's gorgeous home and garden!   \n",
      "13   Stay at an architect's gorgeous home and garden!   \n",
      "15                Alla's Garden House in S.W. Dallas.   \n",
      "7   The Woodlands, BEAUTIFUL HOME, 1 Floor, 2 BT, ...   \n",
      "6                   Home Away from Home in Spring, TX   \n",
      "11                                Waterpark in Dallas   \n",
      "4   New!3BR Fort Worth House near AT&T Stadium w/ ...   \n",
      "8   New!3BR Fort Worth House near AT&T Stadium w/ ...   \n",
      "1   Carriage House on Ranch Homestead in Hill Country   \n",
      "0                           Relaxing house and garden   \n",
      "\n",
      "                                                 Link cosineSim  \n",
      "10  https://www.airbnb.com/rooms/18998883?location...   0.42369  \n",
      "13  https://www.airbnb.com/rooms/18998883?location...   0.42369  \n",
      "15  https://www.airbnb.com/rooms/286328?location=C...  0.407367  \n",
      "7   https://www.airbnb.com/rooms/13065223?location...  0.287556  \n",
      "6   https://www.airbnb.com/rooms/1521227?location=...  0.263551  \n",
      "11  https://www.airbnb.com/rooms/19096810?location...  0.244001  \n",
      "4   https://www.airbnb.com/rooms/19193017?location...  0.224119  \n",
      "8   https://www.airbnb.com/rooms/19193017?location...  0.224119  \n",
      "1   https://www.airbnb.com/rooms/6504290?location=...  0.215188  \n",
      "0   https://www.airbnb.com/rooms/2927741?location=...   0.21033  \n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "SearchEngine2(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define a new score!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will try to create new score to evaluate the compliance of documents with the request.\n",
    "According to the conditions of this task first we should obtain a set of documents using __Search engine__ from the step __3.1__. Then compute the new score for each of this documents. We are not allowed to use the \"description\" and the \"title\" in each doc in the way we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordDict():\n",
    "    with open('wordDict.json', 'r', encoding='latin-1') as fh:\n",
    "        return json.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below computes the new score for the documents obtained from the output of SearchQuery() function. We are using \n",
    "information about the average price per night, number of rooms, city, date of publication and title.\n",
    "New score for each docment is computed with further formula:\n",
    "<br><br>$TotalScore = CityScore*0.1 + PriceScore*0.15 + TitleScore*0.2 + BedScore*0.25 + DateScore*0.3$ <br><br>\n",
    "    __CityScore__ gives weigth of 0.1 to the apartments which are located in the city mentioned in the query. This is a binaru score.<br>\n",
    "    __PriceScore__ computes score according to average price of the apartments:\n",
    "<br>$1-\\frac{averege-price-of-the-apartment-per-night}{sum-of-avprice-for-all-apartemnts-in-output}$<br><br>\n",
    "    __TitleScore__ computes score based on the similarity between the clients request (query) and the documents title:<br><br>$\\frac{len(Query\\, \\cap\\, Title)}{len(Query\\, \\cap\\, Title)}$<br><br>\n",
    "    __BedScore__ is a binary score which adds 0.25 to the total weight if the apartment consist of the necessary number of beds or bedrooms indicated in the query.<br>\n",
    "    __DateScore__ gives more weight to the new publications rather than old:\n",
    "<br><br>$\\frac{index-of-the-publication-in-the-sorted-list-of-publications\\,+\\,1}{len(list-of-publications)}$<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newScore(result, query):\n",
    "    resdict={}\n",
    "    for r in result:# take doc name from result list\n",
    "        with open ('Docs/'+r, \"r\") as myfile: #read string from the tsv fileto list\n",
    "            lst = myfile.readlines()[0].split(sep=\"\\t\") \n",
    "            lst[1]=lst[1].replace('$','').replace(' ','') # preprocess data\n",
    "            lst[2]=lst[2].replace('Studio','0')\n",
    "            if len(lst[1])>0:lst[1]=int(lst[1])\n",
    "            else:lst[1]=0\n",
    "            resdict[r]=[lst[1], lst[2],lst[3],lst[4],lst[8]] #save it in dictionary\n",
    "    # convert dict to Dataframe    \n",
    "    resdf = pd.DataFrame.from_dict(resdict, orient='index', columns=[\"price\",\"bedroom\",\"city\",\"date\",\"title\"])\n",
    "    #create dataframe for score computing\n",
    "    nsdf = pd.DataFrame(0, index=result, columns=[\"price\",\"bedroom\",\"city\",\"date\",\"title\",\"totalscore\"])\n",
    "    newresult=[]\n",
    "    for r in result:#compute all the elements of the totalscore in this loop\n",
    "        #CityScore\n",
    "        if len(set(tokme(resdf.loc[r, \"city\"]))&set(query))>0: nsdf.loc[r, \"city\"] = 0.1\n",
    "        #PriceScore\n",
    "        if resdf.loc[r, \"price\"]!=0: nsdf.loc[r, \"price\"] = (1-resdf.loc[r, \"price\"]/resdf['price'].sum())*0.15 \n",
    "        #TitleScore\n",
    "        nsdf.loc[r, \"title\"] = len(set(query)&set(tokme(resdf.loc[r, \"title\"])))/len(set(query)|set(tokme(resdf.loc[r, \"title\"]))) * 0.2\n",
    "        #BedScore\n",
    "        #Here we should preprocess the query\n",
    "        listquery = [i.replace(\"studio\", \"0\").replace(\"one\", \"1\").replace(\"two\", \"2\").replace(\"three\", \"3\").replace(\"four\", \"4\")\\\n",
    "                     .replace(\"five\", \"5\").replace(\"six\", \"6\").replace(\"seven\", \"7\").replace(\"eight\", \"8\")\\\n",
    "                     .replace(\"nine\", \"9\").replace(\"ten\", \"10\").replace(\"elven\", \"11\").replace(\"twelve\", \"12\")\\\n",
    "                     .replace(\"thirteen\", \"13\") for i in query]\n",
    "        p = re.compile('[0-13]\\sbed*').findall(\" \".join(listquery))\n",
    "        if len(p)>0 and str(resdf.loc[r,\"bedroom\"]) == (p[0].split(\" \"))[0]:  nsdf.loc[r,\"bedroom\"]=0.25\n",
    "        #DateScore\n",
    "        my_date = (resdf.sort_values(\"date\", axis=0, ascending=False, inplace=False, na_position='last'))[\"date\"].tolist()\n",
    "        nsdf.loc[r,\"date\"] = (my_date.index(resdf.loc[r,\"date\"])+1)/len(my_date)*0.3\n",
    "    #TotalScore\n",
    "    nsdf[\"totalscore\"] = nsdf.sum(axis=1)\n",
    "    nsdf = nsdf.sort_values(by='totalscore', ascending=False, kind=\"heapsort\")\n",
    "    #Let`s see what was coputed...\n",
    "    display(HTML(nsdf.to_html()))\n",
    "    result=[(doc, nsdf.loc[doc,\"totalscore\"]) for doc in result]\n",
    "    k=len(result)\n",
    "    if k>5:k=5\n",
    "    return nlargest(k, result, key=lambda e:e[1])\n",
    "#As a result we return a list of tuples which include no more than 5 best matches computed with new score\n",
    "#Sorting is implented with heap data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchQuery(query, wordDict):\n",
    "    result = []\n",
    "    for word in query:\n",
    "        if result != []:\n",
    "            result = list(set(wordDict[word]) & set(result))\n",
    "        else:\n",
    "            result = wordDict[word]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(results):\n",
    "    df = pd.DataFrame(index=range(len(results)), columns=[\"Title\", \"Description\", \"City\", \"Link\"])\n",
    "    for i,r in enumerate(results):\n",
    "        dfd = pd.read_csv('Docs/'+r, sep='\\t', usecols=[8, 5, 3, 9], header=None, names=[\"City\", \"Description\", \"Title\", \"Link\"])\n",
    "        df.loc[i] = dfd.loc[0]\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine(query):\n",
    "    wordDict = loadWordDict()\n",
    "    query = tokme(query)\n",
    "    results = searchQuery(query, wordDict)\n",
    "    newresult = newScore(results, query)\n",
    "    printResult(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>totalscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_15405.tsv</th>\n",
       "      <td>0.107547</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.701992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_16307.tsv</th>\n",
       "      <td>0.107547</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.701992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_11861.tsv</th>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very Large Beautiful En-Suite Master Bedroom</td>\n",
       "      <td>This listing is for the Master En-Suite, featu...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>https://www.airbnb.com/rooms/14500988?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very Large Beautiful En-Suite Master Bedroom</td>\n",
       "      <td>This listing is for the Master En-Suite, featu...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>https://www.airbnb.com/rooms/14500988?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modern Uptown Apartment</td>\n",
       "      <td>This is a new apartment with spacious living r...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>https://www.airbnb.com/rooms/19364154?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"one bedroom beautiful spacious Houston\"\n",
    "try:\n",
    "    SearchEngine(query)\n",
    "except:\n",
    "    print('Sorry...Cannot find apartments for your query.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Step: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Make a nice visualization!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopy\n",
    "import numpy as np\n",
    "from geopy import distance\n",
    "from geopy import Point\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "-95\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#give coordinates as input and a maximum distance from the coordinates (radius)\n",
    "lat = float(input())\n",
    "long = float(input())\n",
    "coordinates = [lat,long]\n",
    "radius = float(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<folium.vector_layers.Circle at 0x1a2842ed30>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a map, with a circle of the given radius, where the center is represented by the coordinates given in input.\n",
    "m = folium.Map(\n",
    "    location=[lat, long]\n",
    ")\n",
    "\n",
    "folium.Marker(\n",
    "    [lat, long], \n",
    "    popup = 'Input Position' , \n",
    "    icon=folium.Icon(icon='icon', color='blue')\n",
    ").add_to(m)\n",
    "\n",
    "folium.Circle(\n",
    "    location = [lat,long], \n",
    "    radius = radius*1000, \n",
    "    color = '#3186cc', \n",
    "    fill = True, \n",
    "    fill_color='#3186cc'\n",
    ").add_to(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1a2842e780>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shows the houses that are inside the circle of the given radius. \n",
    "#We need to calculate the distance between our coordinates and all other positions, \n",
    "#and to add a mark only on the houses inside our radius.\n",
    "Fcsv = pd.read_csv('Airbnb_Texas_Rentals.csv', sep = \",\")\n",
    "Fcsv = Fcsv[np.isfinite(Fcsv['latitude'])]\n",
    "Fcsv = Fcsv[np.isfinite(Fcsv['longitude'])]\n",
    "for index, row in Fcsv.iterrows():\n",
    "        coord = [row['latitude'], row['longitude']]\n",
    "        dist = distance.distance(coord, coordinates).kilometers\n",
    "        if dist <= radius:\n",
    "            folium.Marker(coord, popup = f\"{row['title']}\", icon=folium.Icon(icon='icon', color='blue')).add_to(m)\n",
    "m\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
